{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a322ff02-aa6f-4c3a-9de6-db341501a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import evaluate\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "import loralib as lora\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6840841-62ca-4ca8-ba43-6a4de2da050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    '''\n",
    "    Follow the question answering input format of UnifiedQA: https://arxiv.org/pdf/2005.00700.pdf\n",
    "    '''\n",
    "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.q_len = q_len\n",
    "        self.t_len = t_len\n",
    "        self.data = dataframe\n",
    "        self.question = self.data['question']\n",
    "        self.choices = self.data['choices']\n",
    "        self.label = self.data['label']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.question[idx]\n",
    "        choices = self.choices[idx]\n",
    "        label = int(self.label[idx])\n",
    "        answer = choices[label]\n",
    "        \n",
    "        # Append choices to question following style of UnifiedQA\n",
    "        # question \\n (A) c1 (B) c2 . . .       \n",
    "        letters = ['(A)', '(B)', '(C)', '(D)', '(E)']\n",
    "        question = question + ' \\n'\n",
    "        for i, c in enumerate(choices):\n",
    "            question += f' {letters[i]} {c}'\n",
    "        question_for_tok =  question\n",
    "        answer_for_tok = answer\n",
    "        question_tokenized = self.tokenizer(question_for_tok, max_length=self.q_len, padding=\"max_length\",\n",
    "                                            truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        answer_tokenized = self.tokenizer(answer_for_tok, max_length=self.t_len, padding=\"max_length\", \n",
    "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"decoder_input_ids\": torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"question\": question,\n",
    "            \"ref_answer\": answer,\n",
    "        }\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, tokenizer, train_loader, val_loader, use_lora=False, device='cuda'):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.use_lora = use_lora\n",
    "        self.device = device\n",
    "        self.bleu = evaluate.load(\"google_bleu\")\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        if self.use_lora:\n",
    "            lora.mark_only_lora_as_trainable(self.model)\n",
    "\n",
    "        train_loss = 0\n",
    "        train_batch_count = 0\n",
    "        for batch in tqdm_notebook(self.train_loader, desc=\"Training batches\"):\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"decoder_input_ids\"].to(self.device)\n",
    "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(self.device)\n",
    "    \n",
    "            outputs = self.model(input_ids=input_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 labels=labels,\n",
    "                                 decoder_attention_mask=decoder_attention_mask)\n",
    "    \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs.loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += outputs.loss.item()\n",
    "            train_batch_count += 1\n",
    "\n",
    "        return train_loss / train_batch_count\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        val_batch_count = 0\n",
    "        predicted_answers = []\n",
    "        ref_answers = []\n",
    "        correct_num = 0\n",
    "        total_num = 0\n",
    "        \n",
    "        for batch in tqdm_notebook(self.val_loader, desc=\"Validation batches\"):\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"decoder_input_ids\"].to(self.device)\n",
    "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     labels=labels,\n",
    "                                     decoder_attention_mask=decoder_attention_mask)\n",
    "                val_loss += outputs.loss.item()\n",
    "                val_batch_count += 1\n",
    "    \n",
    "            # Store val outputs and metrics\n",
    "            bs = outputs.logits.shape[0]\n",
    "            for b_idx in range(bs):\n",
    "                logits = outputs.logits[b_idx]\n",
    "                tokens = torch.argmax(logits, dim=1)\n",
    "                end_tok_idx = (tokens == 1).nonzero()\n",
    "    \n",
    "                if end_tok_idx.size(0) > 0:\n",
    "                    end_tok_idx = end_tok_idx[0].item()\n",
    "                    if end_tok_idx+1 < tokens.size(0):\n",
    "                        tokens[end_tok_idx+1:] = 0\n",
    "                \n",
    "                predicted_answer = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "                ref_answer = batch['ref_answer'][b_idx]\n",
    "    \n",
    "                predicted_answers.append(predicted_answer)\n",
    "                ref_answers.append(ref_answer)\n",
    "                if ref_answer in predicted_answer:\n",
    "                    correct_num += 1\n",
    "                total_num += 1\n",
    "        \n",
    "        # Finish calculating val metrics\n",
    "        val_acc = correct_num / total_num\n",
    "        bleu_score = self.bleu.compute(predictions=predicted_answers, references=ref_answers)['google_bleu']\n",
    "\n",
    "        return {'val_loss': val_loss/val_batch_count, 'val_acc': val_acc, 'bleu_score': bleu_score}\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_acc = -float('inf')\n",
    "\n",
    "    def early_stop(self, validation_acc):\n",
    "        if validation_acc > self.min_validation_acc + self.min_delta:\n",
    "            self.min_validation_acc = validation_acc\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def apply_lora(model, num_blocks=12, model_d=768, lora_r=16):\n",
    "    # Apply LoRA to all attention matrices in the transformer block: q,k,v,o\n",
    "    for i in range(num_blocks):\n",
    "        model.encoder.block[i].layer[0].SelfAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        \n",
    "        model.decoder.block[i].layer[0].SelfAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        \n",
    "        model.decoder.block[i].layer[1].EncDecAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d7743f0-2862-4a5a-97b3-ec1f9dd2cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n",
    "use_lora = True\n",
    "lora_r = 16\n",
    "if use_lora:\n",
    "    apply_lora(model, lora_r=lora_r)\n",
    "    model.load_state_dict(torch.load('t5-base.pth'), strict=False)\n",
    "\n",
    "model.load_state_dict(torch.load('results/released_weights/t5_base_lora_scienceqa.pth'), strict=False)\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-3, eps=1e-8, weight_decay=0.0) # For lora, use 3e-3, otherwise 1e-4 learning rate\n",
    "q_len = 512   # Question Length\n",
    "t_len = 64    # Target Length\n",
    "train_batch_size = 16\n",
    "val_batch_size = 8\n",
    "device = 'cuda:0'\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4b097da-099a-49ec-9c1a-5f2216f7f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "#val_file = 'datasets/CommonsenseQA/commonsenseqa_mcq_val.json'\n",
    "#val_file = 'datasets/MedQA/medqa_mcq_val.json'\n",
    "val_file = 'datasets/ScienceQA/scienceqa_mcq_val.json'\n",
    "    \n",
    "with open(val_file) as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "# Create Dataframes\n",
    "val_data = pd.DataFrame(val_data)\n",
    "\n",
    "# Dataloader\n",
    "val_dataset = QA_Dataset(tokenizer, val_data, q_len, t_len)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "trainer = Trainer(model, optimizer, tokenizer, None, val_loader, use_lora=use_lora, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d320025e-07fa-44c0-b08a-ee6ba7c8432d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1ddba837414dc69a22fc183c65e86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation batches:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.010684275164776273,\n",
       " 'val_acc': 0.7933768656716418,\n",
       " 'bleu_score': 0.9690484724535622}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f42b3cb7-0e20-4c4d-bebd-7d4e5d033507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare the motion of three ships. Which ship was moving at the lowest speed? \n",
      " (A) a ship that moved 555kilometers west in 10hours (B) a ship that moved 95kilometers south in 10hours (C) a ship that moved 460kilometers south in 10hours \n",
      "\n",
      "predicted_answer: a ship that moved 95kilometers south in 10hours\n",
      "correct_answer: a ship that moved 95kilometers south in 10hours\n"
     ]
    }
   ],
   "source": [
    "i = 12\n",
    "data = val_dataset.__getitem__(i)\n",
    "input_ids = data['input_ids'].to(device).unsqueeze(0)\n",
    "attention_mask = data['attention_mask'].to(device).unsqueeze(0)\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "predicted_answer = tokenizer.decode(outputs.flatten(), skip_special_tokens=True)\n",
    "\n",
    "print(data['question'], '\\n')\n",
    "print('predicted_answer:', predicted_answer)\n",
    "print('correct_answer:', data['ref_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f9daa-97ed-4c43-b221-8b13b8c19409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
