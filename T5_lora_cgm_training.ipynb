{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93edba44-7a35-4492-a5ce-0ef21dfb13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import evaluate\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "import loralib as lora\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca3f5c0a-195e-4626-80f0-b02d7ce268fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    '''\n",
    "    Follow the question answering input format of UnifiedQA: https://arxiv.org/pdf/2005.00700.pdf\n",
    "    '''\n",
    "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.q_len = q_len\n",
    "        self.t_len = t_len\n",
    "        self.data = dataframe\n",
    "        self.question = self.data['question']\n",
    "        self.choices = self.data['choices']\n",
    "        self.label = self.data['label']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.question[idx]\n",
    "        choices = self.choices[idx]\n",
    "        label = int(self.label[idx])\n",
    "        answer = choices[label]\n",
    "        \n",
    "        # Append choices to question following style of UnifiedQA\n",
    "        # question \\n (A) c1 (B) c2 . . .       \n",
    "        letters = ['(A)', '(B)', '(C)', '(D)', '(E)']\n",
    "        question = question + ' \\n'\n",
    "        for i, c in enumerate(choices):\n",
    "            question += f' {letters[i]} {c}'\n",
    "        question_for_tok =  question\n",
    "        answer_for_tok = answer\n",
    "        question_tokenized = self.tokenizer(question_for_tok, max_length=self.q_len, padding=\"max_length\",\n",
    "                                            truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        answer_tokenized = self.tokenizer(answer_for_tok, max_length=self.t_len, padding=\"max_length\", \n",
    "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"decoder_input_ids\": torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"question\": question,\n",
    "            \"ref_answer\": answer,\n",
    "        }\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, tokenizer, \n",
    "                 train_loader1, val_loader1, \n",
    "                 train_loader2, val_loader2, \n",
    "                 train_loader3, val_loader3, \n",
    "                 device='cuda'):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_loader1 = train_loader1\n",
    "        self.val_loader1 = val_loader1\n",
    "        self.train_loader2 = train_loader2\n",
    "        self.val_loader2 = val_loader2\n",
    "        self.train_loader3 = train_loader3\n",
    "        self.val_loader3 = val_loader3\n",
    "        self.device = device\n",
    "        self.bleu = evaluate.load(\"google_bleu\")\n",
    "\n",
    "        assert len(train_loader1) == len(train_loader2)\n",
    "        assert len(train_loader2) == len(train_loader3)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        train_batch_count = 0\n",
    "        for multi_data_batch in tqdm_notebook(zip(self.train_loader1, self.train_loader2, self.train_loader3), \n",
    "                                              desc=\"Training batches\", total=len(self.train_loader1)):\n",
    "            for batch in multi_data_batch:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"decoder_input_ids\"].to(self.device)\n",
    "                decoder_attention_mask = batch[\"decoder_attention_mask\"].to(self.device)\n",
    "        \n",
    "                outputs = self.model(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     labels=labels,\n",
    "                                     decoder_attention_mask=decoder_attention_mask)\n",
    "        \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs.loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += outputs.loss.item()\n",
    "                train_batch_count += 1\n",
    "\n",
    "        return train_loss / train_batch_count\n",
    "\n",
    "    def validate_epoch(self, dataset_num):\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        val_batch_count = 0\n",
    "        predicted_answers = []\n",
    "        ref_answers = []\n",
    "        correct_num = 0\n",
    "        total_num = 0\n",
    "\n",
    "        if dataset_num == 1:\n",
    "            val_loader = self.val_loader1\n",
    "        elif dataset_num == 2:\n",
    "            val_loader = self.val_loader2\n",
    "        elif dataset_num == 3:\n",
    "            val_loader = self.val_loader3\n",
    "        \n",
    "        for batch in tqdm_notebook(val_loader, desc=\"Validation batches\"):\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"decoder_input_ids\"].to(self.device)\n",
    "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     labels=labels,\n",
    "                                     decoder_attention_mask=decoder_attention_mask)\n",
    "                val_loss += outputs.loss.item()\n",
    "                val_batch_count += 1\n",
    "    \n",
    "            # Store val outputs and metrics\n",
    "            bs = outputs.logits.shape[0]\n",
    "            for b_idx in range(bs):\n",
    "                logits = outputs.logits[b_idx]\n",
    "                tokens = torch.argmax(logits, dim=1)\n",
    "                end_tok_idx = (tokens == 1).nonzero()\n",
    "    \n",
    "                if end_tok_idx.size(0) > 0:\n",
    "                    end_tok_idx = end_tok_idx[0].item()\n",
    "                    if end_tok_idx+1 < tokens.size(0):\n",
    "                        tokens[end_tok_idx+1:] = 0\n",
    "                \n",
    "                predicted_answer = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "                ref_answer = batch['ref_answer'][b_idx]\n",
    "    \n",
    "                predicted_answers.append(predicted_answer)\n",
    "                ref_answers.append(ref_answer)\n",
    "                if ref_answer in predicted_answer:\n",
    "                    correct_num += 1\n",
    "                total_num += 1\n",
    "        \n",
    "        # Finish calculating val metrics\n",
    "        val_acc = correct_num / total_num\n",
    "        bleu_score = self.bleu.compute(predictions=predicted_answers, references=ref_answers)['google_bleu']\n",
    "\n",
    "        return {'val_loss': val_loss/val_batch_count, 'val_acc': val_acc, 'bleu_score': bleu_score}\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_acc = -float('inf')\n",
    "\n",
    "    def early_stop(self, validation_acc):\n",
    "        if validation_acc > self.min_validation_acc + self.min_delta:\n",
    "            self.min_validation_acc = validation_acc\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def apply_lora(model, num_blocks=12, model_d=768, lora_r=16):\n",
    "    # Apply LoRA to all attention matrices in the transformer block: q,k,v,o\n",
    "    for i in range(num_blocks):\n",
    "        model.encoder.block[i].layer[0].SelfAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        \n",
    "        model.decoder.block[i].layer[0].SelfAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        \n",
    "        model.decoder.block[i].layer[1].EncDecAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "\n",
    "def apply_lora_cgm(model,\n",
    "                   commonsense_lora_weights,\n",
    "                   medical_lora_weights,\n",
    "                   science_lora_weights,\n",
    "                   num_blocks=12):\n",
    "    # Apply LoRA to all attention matrices in the transformer block: q,k,v,o\n",
    "    for i in range(num_blocks):\n",
    "        \n",
    "        lora_A_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_A'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_A'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_B'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_B'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_B']]\n",
    "        model.encoder.block[i].layer[0].SelfAttention.q = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_A'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_A'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_B'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_B'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_B']]\n",
    "        model.encoder.block[i].layer[0].SelfAttention.k = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_A'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_A'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_B'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_B'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_B']]\n",
    "        model.encoder.block[i].layer[0].SelfAttention.v = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_A'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_A'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_B'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_B'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_B']]\n",
    "        model.encoder.block[i].layer[0].SelfAttention.o = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_B']]\n",
    "        model.decoder.block[i].layer[0].SelfAttention.q = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_B']]\n",
    "        model.decoder.block[i].layer[0].SelfAttention.k = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_B']]\n",
    "        model.decoder.block[i].layer[0].SelfAttention.v = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_B']]\n",
    "        model.decoder.block[i].layer[0].SelfAttention.o = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_B']]\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.q = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_B']]\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.k = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_B']]\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.v = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_B']]\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.o = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0baca2-a5f3-4f27-94a9-4279beabea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)\n",
    "\n",
    "# Load finetuned LoRA weights\n",
    "commonsense_lora_weights = torch.load('results/commonsense_qa/t5_base_lora_best.pth')\n",
    "medical_lora_weights = torch.load('results/medical_qa/t5_base_lora_best.pth')\n",
    "science_lora_weights = torch.load('results/science_qa/t5_base_lora_best.pth')\n",
    "\n",
    "# Apply lora cgm and reload base model weights\n",
    "apply_lora_cgm(model, commonsense_lora_weights, medical_lora_weights, science_lora_weights)\n",
    "model.load_state_dict(torch.load('t5-base.pth'), strict=False)\n",
    "\n",
    "# Set only mixing module weights to require grad\n",
    "for n, p in model.named_parameters():\n",
    "    if 'context_gated_mixing' in n:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-base')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-3, eps=1e-8, weight_decay=0.0) # For lora, use 3e-3, otherwise 1e-4 learning rate\n",
    "q_len = 512   # Question Length\n",
    "t_len = 64    # Target Length\n",
    "train_batch_size = 16\n",
    "val_batch_size = 8\n",
    "device = 'cuda:0'\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b67cb7-5cc8-4c98-ae10-e54e119fd7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "commonsense_train_file = 'datasets/CommonsenseQA/commonsenseqa_mcq_train.json'\n",
    "commonsense_val_file = 'datasets/CommonsenseQA/commonsenseqa_mcq_val.json'\n",
    "medical_train_file = 'datasets/MedQA/medqa_mcq_train.json'\n",
    "medical_val_file = 'datasets/MedQA/medqa_mcq_val.json'\n",
    "science_train_file = 'datasets/ScienceQA/scienceqa_mcq_train.json'\n",
    "science_val_file = 'datasets/ScienceQA/scienceqa_mcq_val.json'\n",
    "\n",
    "with open(commonsense_train_file) as file:\n",
    "    commonsense_train_data = json.load(file)\n",
    "    \n",
    "with open(commonsense_val_file) as file:\n",
    "    commonsense_val_data = json.load(file)\n",
    "\n",
    "with open(medical_train_file) as file:\n",
    "    medical_train_data = json.load(file)\n",
    "    \n",
    "with open(medical_val_file) as file:\n",
    "    medical_val_data = json.load(file)\n",
    "\n",
    "with open(science_train_file) as file:\n",
    "    science_train_data = json.load(file)\n",
    "    \n",
    "with open(science_val_file) as file:\n",
    "    science_val_data = json.load(file)\n",
    "\n",
    "# Create Dataframes\n",
    "commonsense_train_data = pd.DataFrame(commonsense_train_data)[:256]\n",
    "commonsense_val_data = pd.DataFrame(commonsense_val_data)\n",
    "medical_train_data = pd.DataFrame(medical_train_data)[:256]\n",
    "medical_val_data = pd.DataFrame(medical_val_data)\n",
    "science_train_data = pd.DataFrame(science_train_data)[:256]\n",
    "science_val_data = pd.DataFrame(science_val_data)\n",
    "\n",
    "# Dataset\n",
    "commonsense_train_dataset = QA_Dataset(tokenizer, commonsense_train_data, q_len, t_len)\n",
    "commonsense_val_dataset = QA_Dataset(tokenizer, commonsense_val_data, q_len, t_len)\n",
    "medical_train_dataset = QA_Dataset(tokenizer, medical_train_data, q_len, t_len)\n",
    "medical_val_dataset = QA_Dataset(tokenizer, medical_val_data, q_len, t_len)\n",
    "science_train_dataset = QA_Dataset(tokenizer, science_train_data, q_len, t_len)\n",
    "science_val_dataset = QA_Dataset(tokenizer, science_val_data, q_len, t_len)\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "commonsense_train_loader = DataLoader(commonsense_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "commonsense_val_loader = DataLoader(commonsense_val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "medical_train_loader = DataLoader(medical_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "medical_val_loader = DataLoader(medical_val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "science_train_loader = DataLoader(science_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "science_val_loader = DataLoader(science_val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(model, optimizer, tokenizer, \n",
    "                  commonsense_train_loader, commonsense_val_loader,\n",
    "                  medical_train_loader, medical_val_loader,\n",
    "                  science_train_loader, science_val_loader,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c35281-93d1-4274-8f84-d0f6c4e74047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f3fc6434c6444ebdd3bef925e2f6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation batches:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopper(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initial validation\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m val_metrics1 \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mvalidate_epoch(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m val_metrics2 \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mvalidate_epoch(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m val_metrics3 \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mvalidate_epoch(\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 118\u001b[0m, in \u001b[0;36mTrainer.validate_epoch\u001b[0;34m(self, dataset_num)\u001b[0m\n\u001b[1;32m    115\u001b[0m decoder_attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 118\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    119\u001b[0m                          attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    120\u001b[0m                          labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m    121\u001b[0m                          decoder_attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask)\n\u001b[1;32m    122\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    123\u001b[0m     val_batch_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m   1747\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1748\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1749\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1750\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1751\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m   1752\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1753\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1754\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1755\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1756\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1757\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1758\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1759\u001b[0m )\n\u001b[1;32m   1761\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1112\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     )\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m   1124\u001b[0m         hidden_states,\n\u001b[1;32m   1125\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1126\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m   1127\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1128\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1129\u001b[0m         encoder_decoder_position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m   1130\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m   1131\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m   1132\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m   1133\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1134\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1135\u001b[0m     )\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:695\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m0\u001b[39m](\n\u001b[1;32m    696\u001b[0m     hidden_states,\n\u001b[1;32m    697\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    698\u001b[0m     position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m    699\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    700\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    701\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    702\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    703\u001b[0m )\n\u001b[1;32m    704\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    705\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:602\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    600\u001b[0m ):\n\u001b[1;32m    601\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 602\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    603\u001b[0m         normed_hidden_states,\n\u001b[1;32m    604\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    605\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m    606\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    607\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    608\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    609\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    612\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:521\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# get query states\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m query_states \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq(hidden_states))  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[1;32m    524\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    525\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    526\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/codebases/ContextGatedMOE/loralib/layers_cgm.py:66\u001b[0m, in \u001b[0;36mCGMLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):        \n\u001b[0;32m---> 66\u001b[0m     context_gated_coefs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_gated_mixing(x)\n\u001b[1;32m     67\u001b[0m     context_gated_coefs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(context_gated_coefs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     69\u001b[0m     a \u001b[38;5;241m=\u001b[39m context_gated_coefs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/codebases/ContextGatedMOE/loralib/layers_cgm.py:31\u001b[0m, in \u001b[0;36mContextGatedMixing.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1(x))\n\u001b[0;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# For transformer block inputs, we need to reduce the seq_len dim\u001b[39;00m\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_hydra_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "loss_log = []\n",
    "val_metrics_log = []\n",
    "best_val_acc = -1.0\n",
    "best_model_path = ''\n",
    "early_stopping = EarlyStopper(patience=5, min_delta=1e-3)\n",
    "\n",
    "# Initial validation\n",
    "val_metrics1 = trainer.validate_epoch(1)\n",
    "val_metrics2 = trainer.validate_epoch(2)\n",
    "val_metrics3 = trainer.validate_epoch(3)\n",
    "\n",
    "for val_metrics in [val_metrics1, val_metrics2, val_metrics3]:\n",
    "    val_loss = val_metrics['val_loss']\n",
    "    val_acc = val_metrics['val_acc']\n",
    "    val_bleu = val_metrics['bleu_score']\n",
    "    print(f\"{0}/{num_epochs} -> Validation Acc: {val_acc:.3f} \\tValidation Bleu: {val_bleu:.3f}\")\n",
    "\n",
    "for epoch in range(100, 200):\n",
    "    train_loss = trainer.train_epoch()\n",
    "\n",
    "    val_metrics1 = trainer.validate_epoch(1)\n",
    "    val_metrics2 = trainer.validate_epoch(2)\n",
    "    val_metrics3 = trainer.validate_epoch(3)\n",
    "\n",
    "    avg_val_loss = 0.0\n",
    "    avg_val_acc = 0.0\n",
    "    avg_val_bleu = 0.0\n",
    "    for val_metrics in [val_metrics1, val_metrics2, val_metrics3]:\n",
    "        val_loss = val_metrics['val_loss']\n",
    "        val_acc = val_metrics['val_acc']\n",
    "        val_bleu = val_metrics['bleu_score']\n",
    "        avg_val_loss += val_loss\n",
    "        avg_val_acc += val_acc\n",
    "        avg_val_bleu += val_bleu\n",
    "        print(f\"{epoch+1}/{num_epochs} -> Validation Acc: {val_acc:.3f} \\tValidation Bleu: {val_bleu:.3f}\")\n",
    "\n",
    "    avg_val_loss /= 3\n",
    "    avg_val_acc /= 3\n",
    "    avg_val_bleu /= 3\n",
    "    \n",
    "    loss_log.append((train_loss, avg_val_loss))\n",
    "    val_metrics_log.append((avg_val_acc, avg_val_bleu))\n",
    "    \n",
    "    print('Saving model...')   \n",
    "    checkpoint_path = f'results/cgm_coarse/t5_base_lora_cgm_epoch{epoch+1}.pth'\n",
    "    torch.save(lora.cgm_state_dict(model), checkpoint_path)\n",
    "\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        best_val_acc = avg_val_acc\n",
    "        best_model_path = checkpoint_path\n",
    "    \n",
    "    print(f\"{epoch+1}/{num_epochs} -> Train loss: {train_loss} \\tValidation loss: {avg_val_loss} \" + \\\n",
    "          f\"\\tValidation Acc: {avg_val_acc:.3f} \\tValidation Bleu: {avg_val_bleu:.3f}\")\n",
    "\n",
    "    if early_stopping.early_stop(avg_val_acc):\n",
    "        break\n",
    "\n",
    "torch.save(torch.load(best_model_path), 'results/cgm_coarse/t5_base_lora_cgm_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f5120b2-7612-46b3-a169-7250528feed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/cgm_coarse/t5_base_lora_cgm_epoch151.pth'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32054d9a-b440-4dc0-9e81-f942b891e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of additional parameters: 445104\n",
    "# Can compute metrics using accuracy delta from finetuned and report avereage accuracy delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3d6b63a-3014-408e-afdf-62f3dfd73c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 42-year-old male presents to his primary care physician complaining of abdominal pain. He reports a 5-month history of epigastric pain that improves with meals. He has lost 15 pounds since the pain started. His past medical history is significant for a prolactinoma for which he underwent transphenoidal resection. He drinks alcohol socially and has a 10 pack-year smoking history. His family history is notable for a maternal uncle with a parathyroid adenoma. His temperature is 98.8°F (37.1°C), blood pressure is 125/80 mmHg, pulse is 85/min, and respirations are 18/min. After further workup, the patient is started on octreotide, an analogue of an endogenously produced hormone. When this hormone is produced by the hypothalamus, it has which of the following effects? \n",
      " (A) Decrease production of growth hormone (B) Decrease production of cholecystokinin (C) Decrease production of prolactin (D) Decrease production of gastrin (E) Decrease production of thyrotropin-releasing hormone \n",
      "\n",
      "predicted_answer: Decrease production of cholecystokinin\n",
      "correct_answer: Decrease production of growth hormone\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "data = medical_val_dataset.__getitem__(i)\n",
    "input_ids = data['input_ids'].to(device).unsqueeze(0)\n",
    "attention_mask = data['attention_mask'].to(device).unsqueeze(0)\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "predicted_answer = tokenizer.decode(outputs.flatten(), skip_special_tokens=True)\n",
    "\n",
    "print(data['question'], '\\n')\n",
    "print('predicted_answer:', predicted_answer)\n",
    "print('correct_answer:', data['ref_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabc317-10f8-4246-8dd6-20a3cea2a49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
