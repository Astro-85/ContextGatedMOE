{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edba44-7a35-4492-a5ce-0ef21dfb13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import evaluate\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "import loralib as lora\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f5c0a-195e-4626-80f0-b02d7ce268fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    '''\n",
    "    Follow the question answering input format of UnifiedQA: https://arxiv.org/pdf/2005.00700.pdf\n",
    "    '''\n",
    "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.q_len = q_len\n",
    "        self.t_len = t_len\n",
    "        self.data = dataframe\n",
    "        self.question = self.data['question']\n",
    "        self.choices = self.data['choices']\n",
    "        self.label = self.data['label']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.question[idx]\n",
    "        choices = self.choices[idx]\n",
    "        label = int(self.label[idx])\n",
    "        answer = choices[label]\n",
    "        \n",
    "        # Append choices to question following style of UnifiedQA\n",
    "        # question \\n (A) c1 (B) c2 . . .       \n",
    "        letters = ['(A)', '(B)', '(C)', '(D)', '(E)']\n",
    "        question = question + ' \\n'\n",
    "        for i, c in enumerate(choices):\n",
    "            question += f' {letters[i]} {c}'\n",
    "        question_for_tok =  question\n",
    "        answer_for_tok = answer\n",
    "        question_tokenized = self.tokenizer(question_for_tok, max_length=self.q_len, padding=\"max_length\",\n",
    "                                            truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        answer_tokenized = self.tokenizer(answer_for_tok, max_length=self.t_len, padding=\"max_length\", \n",
    "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"decoder_input_ids\": torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"question\": question,\n",
    "            \"ref_answer\": answer,\n",
    "        }\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, tokenizer, \n",
    "                 train_loader1, val_loader1, \n",
    "                 train_loader2, val_loader2, \n",
    "                 train_loader3, val_loader3, \n",
    "                 device='cuda'):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_loader1 = train_loader1\n",
    "        self.val_loader1 = val_loader1\n",
    "        self.train_loader2 = train_loader2\n",
    "        self.val_loader2 = val_loader2\n",
    "        self.train_loader3 = train_loader3\n",
    "        self.val_loader3 = val_loader3\n",
    "        self.device = device\n",
    "        self.bleu = evaluate.load(\"google_bleu\")\n",
    "\n",
    "        assert len(train_loader1) == len(train_loader2)\n",
    "        assert len(train_loader2) == len(train_loader3)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        train_batch_count = 0\n",
    "        for multi_data_batch in tqdm_notebook(zip(self.train_loader1, self.train_loader2, self.train_loader3), \n",
    "                                              desc=\"Training batches\", total=len(self.train_loader1)):\n",
    "            for batch in multi_data_batch:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"decoder_input_ids\"].to(self.device)\n",
    "                decoder_attention_mask = batch[\"decoder_attention_mask\"].to(self.device)\n",
    "        \n",
    "                outputs = self.model(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     labels=labels,\n",
    "                                     decoder_attention_mask=decoder_attention_mask)\n",
    "        \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs.loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += outputs.loss.item()\n",
    "                train_batch_count += 1\n",
    "\n",
    "        return train_loss / train_batch_count\n",
    "\n",
    "    def validate_epoch(self, dataset_num):\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        val_batch_count = 0\n",
    "        predicted_answers = []\n",
    "        ref_answers = []\n",
    "        correct_num = 0\n",
    "        total_num = 0\n",
    "\n",
    "        if dataset_num == 1:\n",
    "            val_loader = self.val_loader1\n",
    "        elif dataset_num == 2:\n",
    "            val_loader = self.val_loader2\n",
    "        elif dataset_num == 3:\n",
    "            val_loader = self.val_loader3\n",
    "        \n",
    "        for batch in tqdm_notebook(val_loader, desc=\"Validation batches\"):\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"decoder_input_ids\"].to(self.device)\n",
    "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     labels=labels,\n",
    "                                     decoder_attention_mask=decoder_attention_mask)\n",
    "                val_loss += outputs.loss.item()\n",
    "                val_batch_count += 1\n",
    "    \n",
    "            # Store val outputs and metrics\n",
    "            bs = outputs.logits.shape[0]\n",
    "            for b_idx in range(bs):\n",
    "                logits = outputs.logits[b_idx]\n",
    "                tokens = torch.argmax(logits, dim=1)\n",
    "                end_tok_idx = (tokens == 1).nonzero()\n",
    "    \n",
    "                if end_tok_idx.size(0) > 0:\n",
    "                    end_tok_idx = end_tok_idx[0].item()\n",
    "                    if end_tok_idx+1 < tokens.size(0):\n",
    "                        tokens[end_tok_idx+1:] = 0\n",
    "                \n",
    "                predicted_answer = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "                ref_answer = batch['ref_answer'][b_idx]\n",
    "    \n",
    "                predicted_answers.append(predicted_answer)\n",
    "                ref_answers.append(ref_answer)\n",
    "                if ref_answer in predicted_answer:\n",
    "                    correct_num += 1\n",
    "                total_num += 1\n",
    "        \n",
    "        # Finish calculating val metrics\n",
    "        val_acc = correct_num / total_num\n",
    "        bleu_score = self.bleu.compute(predictions=predicted_answers, references=ref_answers)['google_bleu']\n",
    "\n",
    "        return {'val_loss': val_loss/val_batch_count, 'val_acc': val_acc, 'bleu_score': bleu_score}\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_acc = -float('inf')\n",
    "\n",
    "    def early_stop(self, validation_acc):\n",
    "        if validation_acc > self.min_validation_acc + self.min_delta:\n",
    "            self.min_validation_acc = validation_acc\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def apply_lora(model, num_blocks=12, model_d=768, lora_r=16):\n",
    "    # Apply LoRA to all attention matrices in the transformer block: q,k,v,o\n",
    "    for i in range(num_blocks):\n",
    "        model.encoder.block[i].layer[0].SelfAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        \n",
    "        model.decoder.block[i].layer[0].SelfAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        \n",
    "        model.decoder.block[i].layer[1].EncDecAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "\n",
    "def apply_lora_cgm(model,\n",
    "                   commonsense_lora_weights,\n",
    "                   medical_lora_weights,\n",
    "                   science_lora_weights,\n",
    "                   num_blocks=12):\n",
    "    # Apply LoRA to all attention matrices in the transformer block: q,k,v,o\n",
    "    for i in range(num_blocks):\n",
    "        \n",
    "        lora_A_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_A'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_A'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_B'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_B'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.q.lora_B']]\n",
    "        model.encoder.block[i].layer[0].SelfAttention.q = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_A'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_A'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_B'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_B'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.k.lora_B']]\n",
    "        model.encoder.block[i].layer[0].SelfAttention.k = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_A'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_A'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_B'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_B'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.v.lora_B']]\n",
    "        model.encoder.block[i].layer[0].SelfAttention.v = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_A'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_A'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_B'],\n",
    "                          medical_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_B'],\n",
    "                          science_lora_weights[f'encoder.block.{i}.layer.0.SelfAttention.o.lora_B']]\n",
    "        model.encoder.block[i].layer[0].SelfAttention.o = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.q.lora_B']]\n",
    "        model.decoder.block[i].layer[0].SelfAttention.q = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.k.lora_B']]\n",
    "        model.decoder.block[i].layer[0].SelfAttention.k = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.v.lora_B']]\n",
    "        model.decoder.block[i].layer[0].SelfAttention.v = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.0.SelfAttention.o.lora_B']]\n",
    "        model.decoder.block[i].layer[0].SelfAttention.o = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.q.lora_B']]\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.q = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.k.lora_B']]\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.k = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.v.lora_B']]\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.v = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)\n",
    "\n",
    "        lora_A_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_A'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_A'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_A']]\n",
    "        lora_B_weights = [commonsense_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_B'],\n",
    "                          medical_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_B'],\n",
    "                          science_lora_weights[f'decoder.block.{i}.layer.1.EncDecAttention.o.lora_B']]\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.o = lora.CGMLinear(lora_A_weights, lora_B_weights, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0baca2-a5f3-4f27-94a9-4279beabea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)\n",
    "\n",
    "# Load finetuned LoRA weights\n",
    "commonsense_lora_weights = torch.load('results/commonsense_qa/t5_base_lora_best.pth')\n",
    "medical_lora_weights = torch.load('results/medical_qa/t5_base_lora_best.pth')\n",
    "science_lora_weights = torch.load('results/science_qa/t5_base_lora_best.pth')\n",
    "\n",
    "# Apply lora cgm and reload base model weights\n",
    "apply_lora_cgm(model, commonsense_lora_weights, medical_lora_weights, science_lora_weights)\n",
    "model.load_state_dict(torch.load('t5-base.pth'), strict=False)\n",
    "\n",
    "# Set only mixing module weights to require grad\n",
    "for n, p in model.named_parameters():\n",
    "    if 'context_gated_mixing' in n:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-base')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-3, eps=1e-8, weight_decay=0.0) # For lora, use 3e-3, otherwise 1e-4 learning rate\n",
    "q_len = 512   # Question Length\n",
    "t_len = 64    # Target Length\n",
    "train_batch_size = 16\n",
    "val_batch_size = 8\n",
    "device = 'cuda:0'\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b67cb7-5cc8-4c98-ae10-e54e119fd7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "commonsense_train_file = 'datasets/CommonsenseQA/commonsenseqa_mcq_train.json'\n",
    "commonsense_val_file = 'datasets/CommonsenseQA/commonsenseqa_mcq_val.json'\n",
    "medical_train_file = 'datasets/MedQA/medqa_mcq_train.json'\n",
    "medical_val_file = 'datasets/MedQA/medqa_mcq_val.json'\n",
    "science_train_file = 'datasets/ScienceQA/scienceqa_mcq_train.json'\n",
    "science_val_file = 'datasets/ScienceQA/scienceqa_mcq_val.json'\n",
    "\n",
    "with open(commonsense_train_file) as file:\n",
    "    commonsense_train_data = json.load(file)\n",
    "    \n",
    "with open(commonsense_val_file) as file:\n",
    "    commonsense_val_data = json.load(file)\n",
    "\n",
    "with open(medical_train_file) as file:\n",
    "    medical_train_data = json.load(file)\n",
    "    \n",
    "with open(medical_val_file) as file:\n",
    "    medical_val_data = json.load(file)\n",
    "\n",
    "with open(science_train_file) as file:\n",
    "    science_train_data = json.load(file)\n",
    "    \n",
    "with open(science_val_file) as file:\n",
    "    science_val_data = json.load(file)\n",
    "\n",
    "# Create Dataframes\n",
    "commonsense_train_data = pd.DataFrame(commonsense_train_data)[:256]\n",
    "commonsense_val_data = pd.DataFrame(commonsense_val_data)\n",
    "medical_train_data = pd.DataFrame(medical_train_data)[:256]\n",
    "medical_val_data = pd.DataFrame(medical_val_data)\n",
    "science_train_data = pd.DataFrame(science_train_data)[:256]\n",
    "science_val_data = pd.DataFrame(science_val_data)\n",
    "\n",
    "# Dataset\n",
    "commonsense_train_dataset = QA_Dataset(tokenizer, commonsense_train_data, q_len, t_len)\n",
    "commonsense_val_dataset = QA_Dataset(tokenizer, commonsense_val_data, q_len, t_len)\n",
    "medical_train_dataset = QA_Dataset(tokenizer, medical_train_data, q_len, t_len)\n",
    "medical_val_dataset = QA_Dataset(tokenizer, medical_val_data, q_len, t_len)\n",
    "science_train_dataset = QA_Dataset(tokenizer, science_train_data, q_len, t_len)\n",
    "science_val_dataset = QA_Dataset(tokenizer, science_val_data, q_len, t_len)\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "commonsense_train_loader = DataLoader(commonsense_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "commonsense_val_loader = DataLoader(commonsense_val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "medical_train_loader = DataLoader(medical_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "medical_val_loader = DataLoader(medical_val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "science_train_loader = DataLoader(science_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "science_val_loader = DataLoader(science_val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(model, optimizer, tokenizer, \n",
    "                  commonsense_train_loader, commonsense_val_loader,\n",
    "                  medical_train_loader, medical_val_loader,\n",
    "                  science_train_loader, science_val_loader,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c35281-93d1-4274-8f84-d0f6c4e74047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "loss_log = []\n",
    "val_metrics_log = []\n",
    "best_val_acc = -1.0\n",
    "best_model_path = ''\n",
    "early_stopping = EarlyStopper(patience=5, min_delta=1e-3)\n",
    "\n",
    "# Initial validation\n",
    "val_metrics1 = trainer.validate_epoch(1)\n",
    "val_metrics2 = trainer.validate_epoch(2)\n",
    "val_metrics3 = trainer.validate_epoch(3)\n",
    "\n",
    "for val_metrics in [val_metrics1, val_metrics2, val_metrics3]:\n",
    "    val_loss = val_metrics['val_loss']\n",
    "    val_acc = val_metrics['val_acc']\n",
    "    val_bleu = val_metrics['bleu_score']\n",
    "    print(f\"{0}/{num_epochs} -> Validation Acc: {val_acc:.3f} \\tValidation Bleu: {val_bleu:.3f}\")\n",
    "\n",
    "for epoch in range(100, 200):\n",
    "    train_loss = trainer.train_epoch()\n",
    "\n",
    "    val_metrics1 = trainer.validate_epoch(1)\n",
    "    val_metrics2 = trainer.validate_epoch(2)\n",
    "    val_metrics3 = trainer.validate_epoch(3)\n",
    "\n",
    "    avg_val_loss = 0.0\n",
    "    avg_val_acc = 0.0\n",
    "    avg_val_bleu = 0.0\n",
    "    for val_metrics in [val_metrics1, val_metrics2, val_metrics3]:\n",
    "        val_loss = val_metrics['val_loss']\n",
    "        val_acc = val_metrics['val_acc']\n",
    "        val_bleu = val_metrics['bleu_score']\n",
    "        avg_val_loss += val_loss\n",
    "        avg_val_acc += val_acc\n",
    "        avg_val_bleu += val_bleu\n",
    "        print(f\"{epoch+1}/{num_epochs} -> Validation Acc: {val_acc:.3f} \\tValidation Bleu: {val_bleu:.3f}\")\n",
    "\n",
    "    avg_val_loss /= 3\n",
    "    avg_val_acc /= 3\n",
    "    avg_val_bleu /= 3\n",
    "    \n",
    "    loss_log.append((train_loss, avg_val_loss))\n",
    "    val_metrics_log.append((avg_val_acc, avg_val_bleu))\n",
    "    \n",
    "    print('Saving model...')   \n",
    "    checkpoint_path = f'results/cgm_coarse/t5_base_lora_cgm_epoch{epoch+1}.pth'\n",
    "    torch.save(lora.cgm_state_dict(model), checkpoint_path)\n",
    "\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        best_val_acc = avg_val_acc\n",
    "        best_model_path = checkpoint_path\n",
    "    \n",
    "    print(f\"{epoch+1}/{num_epochs} -> Train loss: {train_loss} \\tValidation loss: {avg_val_loss} \" + \\\n",
    "          f\"\\tValidation Acc: {avg_val_acc:.3f} \\tValidation Bleu: {avg_val_bleu:.3f}\")\n",
    "\n",
    "    if early_stopping.early_stop(avg_val_acc):\n",
    "        break\n",
    "\n",
    "torch.save(torch.load(best_model_path), 'results/cgm_coarse/t5_base_lora_cgm_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d6b63a-3014-408e-afdf-62f3dfd73c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "data = medical_val_dataset.__getitem__(i)\n",
    "input_ids = data['input_ids'].to(device).unsqueeze(0)\n",
    "attention_mask = data['attention_mask'].to(device).unsqueeze(0)\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "predicted_answer = tokenizer.decode(outputs.flatten(), skip_special_tokens=True)\n",
    "\n",
    "print(data['question'], '\\n')\n",
    "print('predicted_answer:', predicted_answer)\n",
    "print('correct_answer:', data['ref_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabc317-10f8-4246-8dd6-20a3cea2a49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
