{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import evaluate\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "import loralib as lora\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    '''\n",
    "    Follow the question answering input format of UnifiedQA: https://arxiv.org/pdf/2005.00700.pdf\n",
    "    '''\n",
    "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.q_len = q_len\n",
    "        self.t_len = t_len\n",
    "        self.data = dataframe\n",
    "        self.question = self.data['question']\n",
    "        self.choices = self.data['choices']\n",
    "        self.label = self.data['label']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.question[idx]\n",
    "        choices = self.choices[idx]\n",
    "        label = int(self.label[idx])\n",
    "        answer = choices[label]\n",
    "        \n",
    "        # Append choices to question following style of UnifiedQA\n",
    "        # question \\n (A) c1 (B) c2 . . .       \n",
    "        letters = ['(A)', '(B)', '(C)', '(D)', '(E)']\n",
    "        question = question + ' \\n'\n",
    "        for i, c in enumerate(choices):\n",
    "            question += f' {letters[i]} {c}'\n",
    "        question_for_tok =  question\n",
    "        answer_for_tok = answer\n",
    "        question_tokenized = self.tokenizer(question_for_tok, max_length=self.q_len, padding=\"max_length\",\n",
    "                                            truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        answer_tokenized = self.tokenizer(answer_for_tok, max_length=self.t_len, padding=\"max_length\", \n",
    "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"decoder_input_ids\": torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"question\": question,\n",
    "            \"ref_answer\": answer,\n",
    "        }\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, tokenizer, train_loader, val_loader, use_lora=False, device='cuda'):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.use_lora = use_lora\n",
    "        self.device = device\n",
    "        self.bleu = evaluate.load(\"google_bleu\")\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        if self.use_lora:\n",
    "            lora.mark_only_lora_as_trainable(self.model)\n",
    "\n",
    "        train_loss = 0\n",
    "        train_batch_count = 0\n",
    "        for batch in tqdm_notebook(self.train_loader, desc=\"Training batches\"):\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"decoder_input_ids\"].to(self.device)\n",
    "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(self.device)\n",
    "    \n",
    "            outputs = self.model(input_ids=input_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 labels=labels,\n",
    "                                 decoder_attention_mask=decoder_attention_mask)\n",
    "    \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs.loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += outputs.loss.item()\n",
    "            train_batch_count += 1\n",
    "\n",
    "        return train_loss / train_batch_count\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        val_batch_count = 0\n",
    "        predicted_answers = []\n",
    "        ref_answers = []\n",
    "        correct_num = 0\n",
    "        total_num = 0\n",
    "        \n",
    "        for batch in tqdm_notebook(self.val_loader, desc=\"Validation batches\"):\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"decoder_input_ids\"].to(self.device)\n",
    "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     labels=labels,\n",
    "                                     decoder_attention_mask=decoder_attention_mask)\n",
    "                val_loss += outputs.loss.item()\n",
    "                val_batch_count += 1\n",
    "    \n",
    "            # Store val outputs and metrics\n",
    "            bs = outputs.logits.shape[0]\n",
    "            for b_idx in range(bs):\n",
    "                logits = outputs.logits[b_idx]\n",
    "                tokens = torch.argmax(logits, dim=1)\n",
    "                end_tok_idx = (tokens == 1).nonzero()\n",
    "    \n",
    "                if end_tok_idx.size(0) > 0:\n",
    "                    end_tok_idx = end_tok_idx[0].item()\n",
    "                    if end_tok_idx+1 < tokens.size(0):\n",
    "                        tokens[end_tok_idx+1:] = 0\n",
    "                \n",
    "                predicted_answer = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "                ref_answer = batch['ref_answer'][b_idx]\n",
    "    \n",
    "                predicted_answers.append(predicted_answer)\n",
    "                ref_answers.append(ref_answer)\n",
    "                if ref_answer in predicted_answer:\n",
    "                    correct_num += 1\n",
    "                total_num += 1\n",
    "        \n",
    "        # Finish calculating val metrics\n",
    "        val_acc = correct_num / total_num\n",
    "        bleu_score = self.bleu.compute(predictions=predicted_answers, references=ref_answers)['google_bleu']\n",
    "\n",
    "        return {'val_loss': val_loss/val_batch_count, 'val_acc': val_acc, 'bleu_score': bleu_score}\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_acc = -float('inf')\n",
    "\n",
    "    def early_stop(self, validation_acc):\n",
    "        if validation_acc > self.min_validation_acc + self.min_delta:\n",
    "            self.min_validation_acc = validation_acc\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def apply_lora(model, num_blocks=12, model_d=768, lora_r=16):\n",
    "    # Apply LoRA to all attention matrices in the transformer block: q,k,v,o\n",
    "    for i in range(num_blocks):\n",
    "        model.encoder.block[i].layer[0].SelfAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        \n",
    "        model.decoder.block[i].layer[0].SelfAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[0].SelfAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        \n",
    "        model.decoder.block[i].layer[1].EncDecAttention.q = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.k = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.v = lora.Linear(model_d, model_d, r=lora_r, bias=False)\n",
    "        model.decoder.block[i].layer[1].EncDecAttention.o = lora.Linear(model_d, model_d, r=lora_r, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6708244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n",
    "use_lora = True\n",
    "lora_r = 16\n",
    "if use_lora:\n",
    "    apply_lora(model, lora_r=lora_r)\n",
    "    model.load_state_dict(torch.load('t5-base.pth'), strict=False)\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-3, eps=1e-8, weight_decay=0.0) # For lora, use 3e-3, otherwise 1e-4 learning rate\n",
    "q_len = 512   # Question Length\n",
    "t_len = 64    # Target Length\n",
    "train_batch_size = 16\n",
    "val_batch_size = 8\n",
    "device = 'cuda:0'\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train_file = 'datasets/CommonsenseQA/commonsenseqa_mcq_train.json'\n",
    "val_file = 'datasets/CommonsenseQA/commonsenseqa_mcq_val.json'\n",
    "#train_file = 'datasets/MedQA/medqa_mcq_train.json'\n",
    "#val_file = 'datasets/MedQA/medqa_mcq_val.json'\n",
    "#train_file = 'datasets/ScienceQA/scienceqa_mcq_train.json'\n",
    "#val_file = 'datasets/ScienceQA/scienceqa_mcq_val.json'\n",
    "\n",
    "with open(train_file) as file:\n",
    "    train_data = json.load(file)\n",
    "    \n",
    "with open(val_file) as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "# Create Dataframes\n",
    "train_data = pd.DataFrame(train_data)\n",
    "val_data = pd.DataFrame(val_data)\n",
    "\n",
    "# Dataloader\n",
    "train_dataset = QA_Dataset(tokenizer, train_data, q_len, t_len)\n",
    "val_dataset = QA_Dataset(tokenizer, val_data, q_len, t_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(model, optimizer, tokenizer, train_loader, val_loader, use_lora=use_lora, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496cd9e-cbc1-4ff6-bca8-8204296ad06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "loss_log = []\n",
    "val_metrics_log = []\n",
    "best_val_acc = -1.0\n",
    "best_model_path = ''\n",
    "early_stopping = EarlyStopper(patience=3, min_delta=1e-3)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = trainer.train_epoch()\n",
    "    val_metrics = trainer.validate_epoch()\n",
    "\n",
    "    val_loss = val_metrics['val_loss']\n",
    "    val_acc = val_metrics['val_acc']\n",
    "    val_bleu = val_metrics['bleu_score']\n",
    "    \n",
    "    loss_log.append((train_loss, val_loss))\n",
    "    val_metrics_log.append((val_acc, val_bleu))\n",
    "\n",
    "    print('Saving model...')   \n",
    "    checkpoint_path = f'results/medical_qa/t5_base_lora_epoch{epoch+1}.pth'\n",
    "    if use_lora:\n",
    "        torch.save(lora.lora_state_dict(model), checkpoint_path)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_path = checkpoint_path\n",
    "    \n",
    "    print(f\"{epoch+1}/{num_epochs} -> Train loss: {train_loss} \\tValidation loss: {val_loss} \" + \\\n",
    "          f\"\\tValidation Acc: {val_acc:.3f} \\tValidation Bleu: {val_bleu:.3f}\")\n",
    "\n",
    "    if early_stopping.early_stop(val_acc):\n",
    "        break\n",
    "\n",
    "# Save best\n",
    "if use_lora:\n",
    "    torch.save(torch.load(best_model_path), 'results/medical_qa/t5_base_lora_best.pth')\n",
    "else:\n",
    "    torch.save(torch.load(best_model_path), 'results/medical_qa/t5_base_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79dff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation curves\n",
    "train_loss = [x[0] for x in loss_log]\n",
    "val_loss = [x[1] for x in loss_log]\n",
    "epochs = range(1, len(loss_log)+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, train_loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('MedicalQA T5-Base LoRA Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Display and save the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"medicalqa_t5_base_lora_train_val_loss.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf4af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation metrics\n",
    "acc = [x[0] for x in val_metrics_log]\n",
    "bleu = [x[1] for x in val_metrics_log]\n",
    "epochs = range(1, len(loss_log)+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, acc, label='Validation Accuracy')\n",
    "plt.plot(epochs, bleu, label='Validation Bleu')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('MedicalQA T5-Base LoRA Validation Metrics')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy and Bleu Score')\n",
    " \n",
    "# Display and save the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"medicalqa_t5_base_lora_val_metrics.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cdbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Qualitative examples\n",
    "i = 1\n",
    "data = val_dataset.__getitem__(i)\n",
    "input_ids = data['input_ids'].to(device).unsqueeze(0)\n",
    "attention_mask = data['attention_mask'].to(device).unsqueeze(0)\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "predicted_answer = tokenizer.decode(outputs.flatten(), skip_special_tokens=True)\n",
    "\n",
    "print(data['question'], '\\n')\n",
    "print('predicted_answer:', predicted_answer)\n",
    "print('correct_answer:', data['ref_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6ad4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
